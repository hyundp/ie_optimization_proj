{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n    더 나아가서,\\n    1. multi agent를 사용해야하나?\\n    \\n    state 설정\\n    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \\n    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\\n    따라서 state는 (729, 6)의 크기를 갖는다.\\n    \\n    deep q 이유\\n    만약 일반 q learning을 쓸 때, \\n    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\\n    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    정리\n",
    "    1. agent를 통해서 policy(랜덤값이 앱실론보다 작으면 랜덤 인덱스와 랜덤 action을, \n",
    "                            그렇지 않으면 내부 신경망(LSTM)에 state값을 넣어 \n",
    "                            가능한 모든 action에 대한 action_values(Q_values) 중 가장 큰 값에 해당하는 인덱스와 그 값을 반환한다.\n",
    "                            이때 사용되는 내부 신경망은 main_network이다.)\n",
    "        에 맞는 action_index와 action_value를 찾아낸다.\n",
    "    2. 찾아낸 action_index와 action_value를 env에 넣어서 RF 모델을 돌린 뒤 next_state(다음 action 집합)와 reward(정확도)를 찾아낸다.\n",
    "    3. memory(replay buffer)에 현재 state와 action_index, 그리고 위에서 도출된 next_state, reward를 넣는다.\n",
    "    4. replay buffer에서 원하는 batch size만큼 sample(state, action_index, reward, next_state)을 뽑아낸다.\n",
    "    5. sample의 state를 main_network에 넣어서 도출된 값들 중 action index에 맞는 값을 뽑고 이를 main Q 값으로 한다.\n",
    "    6. sample의 next_state를 target_network에 넣어서 값을 도출한 뒤, 그 중 가장 큰 값을 찾아낸다.\n",
    "    7. 위에서 찾아낸 값을 Q라고 한다면, reward + gamma(discount factor)*Q = target Q로 정한다.\n",
    "    8. mainQ와 targetQ를 loss function에 넣어서 내부 신경망 파라미터를 역전파로 업데이트 한다. 이때 옵티마이저도 사용된다.\n",
    "    9. 이 과정을 반복하여 main Q가 target Q에 가까워질 수 있게 한다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"    \n",
    "    더 나아가서,\n",
    "    1. multi agent를 사용해야하나?\n",
    "    \n",
    "    state 설정\n",
    "    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \n",
    "    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\n",
    "    따라서 state는 (729, 6)의 크기를 갖는다.\n",
    "    \n",
    "    deep q 이유\n",
    "    만약 일반 q learning을 쓸 때, \n",
    "    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\n",
    "    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "from itertools import product\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "class h2y2_RF_Model():\n",
    "    def __init__(self, cur_hyperparameter):\n",
    "        self.model = RandomForestClassifier(**cur_hyperparameter, random_state=42)\n",
    "        self.data = load_breast_cancer()\n",
    "        x_data = pd.DataFrame(self.data.data, columns=self.data.feature_names)\n",
    "        y_data = self.data.target\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.fit(self.train_x, self.train_y)\n",
    "        predict = self.model.predict(self.test_x)\n",
    "        return accuracy_score(self.test_y, predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_env:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initial\n",
    "        self.comb_config = [[-50,0,50], [-2,0,2], [-1,0,1], [-1,0,1], [-0.1,0,0.1], [-0.1,0,0.1]]\n",
    "        self.hyperparameter_list = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'min_impurity_decrease']\n",
    "        self.epsilon = 1e-3\n",
    "    \n",
    "    # 하이퍼파라미터의 범위를 제한해주는 함수 ver1\n",
    "    def check_bound(self, comb):        \n",
    "        comb_sum = comb[1] + comb[2]\n",
    "        if comb[0] == 'n_estimators':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'max_depth':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_samples_leaf':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_samples_split':\n",
    "            if comb_sum > 1:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_weight_fraction_leaf':\n",
    "            if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                return float(comb_sum)\n",
    "        elif comb[0] == 'min_impurity_decrease':\n",
    "            if comb_sum >= 0 and comb_sum <= 1:\n",
    "                return float(comb_sum)\n",
    "        return comb[1]\n",
    "    \n",
    "    # 하이퍼파라미터의 범위를 제한해주는 함수 ver2\n",
    "    def check_bound_ver2(self, comb):\n",
    "        sample_state = []\n",
    "        for i in range(len(comb[2])):\n",
    "            comb_sum = comb[1] + comb[2][i]\n",
    "            if comb[0] == 'n_estimators':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'max_depth':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_samples_leaf':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_samples_split':\n",
    "                if comb_sum > 1:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_weight_fraction_leaf':\n",
    "                if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                    sample_state.append(float(comb_sum))\n",
    "            elif comb[0] == 'min_impurity_decrease':\n",
    "                if comb_sum >= 0 and comb_sum <= 1:\n",
    "                    sample_state.append(float(comb_sum))\n",
    "            else:\n",
    "                sample_state.append(comb[1])\n",
    "        return sample_state\n",
    "    \n",
    "    # hyper-parameter vector를 받아서 다음으로 가능한 모든 조합을 반환해주는 함수\n",
    "    def make_state(self, cur_comb):\n",
    "        comb = list(product(*self.comb_config))\n",
    "        state = [tuple(map(self.check_bound, zip(self.hyperparameter_list, cur_comb, tuple_comb))) for tuple_comb in comb]\n",
    "        # print(state)\n",
    "        return state\n",
    "        \n",
    "    # q_value로 도출된 action_index가 어떤 observation을 가리키는지 확인한다.\n",
    "    def mapping_action(self, state, action_index):\n",
    "        all_state = list(product(*state))\n",
    "        # print(len(all_state))\n",
    "        return all_state[action_index]\n",
    "        \n",
    "    # hyper-parameter vector를 받아서 파라미터마다 가능한 값을 2차원으로 반환해주는 함수 ex)[[90, 100, 110], [2, 4, 6], [20, 25, 30], ...]\n",
    "    def make_state_ver2(self, cur_comb):\n",
    "        state = []\n",
    "        for name, cur, comb in zip(self.hyperparameter_list, cur_comb, self.comb_config):\n",
    "            state.append(self.check_bound_ver2([name,cur,comb]))\n",
    "        # print(state)\n",
    "        return state\n",
    "    \n",
    "    # env의 초기 state 설정, state는 hyper-parameter의 가능한 모든 조합으로 정의한다.\n",
    "    def reset(self):\n",
    "        init_hp = [random.randint(50, 100), random.randint(50, 100), random.randint(2, 10), random.randint(1, 10), random.random()/2, random.random()]\n",
    "        state = self.make_state(init_hp)\n",
    "        return state\n",
    "        \n",
    "    # action을 넣어서 next_state와 reward를 반환하는 함수\n",
    "    def step(self, state, action_index):\n",
    "        done = 0\n",
    "        cur_comb = state[action_index]\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state(cur_comb)\n",
    "        if reward == 1-self.epsilon:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # action을 넣어서 next_state와 reward를 반환하는 함수 ver2\n",
    "    def step_ver2(self, state, action_index):\n",
    "        done = 0\n",
    "        cur_comb = self.mapping_action(state, action_index) # state에 맞게 현재 조합 매핑\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state_ver2(cur_comb)\n",
    "        if reward == 1-self.epsilon:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.99 # discount factor\n",
    "        self.t_step = 0\n",
    "        self.learn_freq = 4\n",
    "        self.target_update_freq = 2000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 6\n",
    "\n",
    "        self.main_network = Network(input_size=(729,6), out_size=729).float().to(device)\n",
    "        self.target_network = Network(input_size=(729,6), out_size=729).float().to(device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "        self.hidden_state, self.cell_state = self.main_network.init_hidden_states(bsize=1)\n",
    "        self.memory = ReplayBuffer(action_size = self.action_size, buffer_size = 10000, batch_size = self.batch_size, seed=42)\n",
    "        self.optimizer = optim.Adam(self.main_network.parameters(), lr = 0.01)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, eps=0.):\n",
    "        # \"내부의 신경망에 state를 넣어 모든 q_value를 뽑고, argmax로 선택\"\n",
    "        \n",
    "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0).to(device)\n",
    "        self.main_network.eval()\n",
    "        with torch.no_grad(): # 연산속도 증가\n",
    "            action_values, _ = self.main_network.forward(state, bsize=1, time_step=1, hidden_state=self.hidden_state, cell_state=self.cell_state)\n",
    "            # print(type(action_values)) # tuple\n",
    "        self.main_network.train()\n",
    "    \n",
    "        # q_value를 최대로 만드는 action의 인덱스를 선택한다.\n",
    "        if random.random() > eps:\n",
    "            max_index = torch.argmax(action_values)\n",
    "            return max_index\n",
    "        else:\n",
    "            random_index = random.choice(np.arange(self.action_size))\n",
    "            return random_index\n",
    "            \n",
    "    def step(self, state, action_index, reward, next_state, done):\n",
    "        # 메모리에 현재의 state, action_index, reward, next_state, done을 추가한다.\n",
    "        self.t_step += 1 \n",
    "        # print(self.t_step) # t_step은 episode가 바뀌어도 유지된다.\n",
    "        self.memory.add(state, action_index, reward, next_state, done)\n",
    "        \n",
    "        # target_update로 정해둔 step마다, target network의 파라미터를 업데이트 한다.\n",
    "        if (self.t_step % self.target_update_freq) == 0:\n",
    "            self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "            \n",
    "        # learn_freq로 정해둔 step마다, batch_size만큼의 샘플을 가지고 main_network를 학습시킨다.\n",
    "        if (self.t_step % self.learn_freq) == 0:\n",
    "            # batch_size만큼의 sample이 memory에 있으면 학습을 실행한다.\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "    \n",
    "    # main_network의 파라미터를 학습하는 함수\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, reward, next_states, dones = experiences\n",
    "        hidden_batch, cell_batch = self.main_network.init_hidden_states(bsize=self.batch_size)\n",
    "\n",
    "        # target_network를 통해 next_state에 대한 q_value 값을 도출하고, 그 중 max인 값을 선택한다.\n",
    "        Q_targets_next, _ = self.target_network.forward(next_states,bsize=self.batch_size, time_step=1, hidden_state=hidden_batch, cell_state=cell_batch)\n",
    "        Q_targets_next_max, __ = Q_targets_next.detach().max(dim=1)\n",
    "        Q_targets_next_max = Q_targets_next_max.view(-1,1)\n",
    "        # print(Q_targets_next_max.shape) # torch.Size([23328, 1]) -> torch.Size([32, 1])이 나와야함. -> 완료\n",
    "        \n",
    "        # q_value_target을 계산한다.\n",
    "        Q_targets = reward + (gamma * Q_targets_next_max)\n",
    "        # print(Q_targets.shape) # (32,1)\n",
    "\n",
    "        # main_network를 통해 현재 state와 action 대한 q_value를 도출한다.\n",
    "        Q_expected, _ = self.main_network.forward(states, bsize=self.batch_size, time_step=1, hidden_state=hidden_batch, cell_state=cell_batch)\n",
    "        # print(Q_expected.shape) # (32,729)\n",
    "        Q_expected_action = Q_expected.gather(dim=1, index = actions)\n",
    "        # print(Q_expected_action.shape) # (32,1)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected_action, Q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.action_size = action_size #각각의 action의 차원\n",
    "        self.memory = deque(maxlen=buffer_size)  #버퍼의 최대 크기\n",
    "        self.batch_size = batch_size # 배치 사이즈\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed) # 랜덤 시드\n",
    "    \n",
    "    # states, actions, rewards, next_states, done을 replay memory에 저장하는 함수\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    # replay memory에서 이전 과정에서 저장된 states, actions, rewards, next_states, done을 랜덤 추출하는 함수\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([torch.as_tensor(e.state).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        # print(states.shape) # (32*729,6)\n",
    "        actions = torch.from_numpy(np.vstack([torch.as_tensor(e.action).cpu() for e in experiences if e is not None])).long().to(device)\n",
    "        # print(actions.shape) # (32,1)\n",
    "        rewards = torch.from_numpy(np.vstack([torch.as_tensor(e.reward).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        # print(rewards.shape) # (32,1)\n",
    "        next_states = torch.from_numpy(np.vstack([torch.as_tensor(e.next_state).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([torch.as_tensor(e.done).cpu() for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        # print(dones.shape) # (32,1)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru도 비교해보면 좋다.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,out_size):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv1d(in_channels=1,out_channels=32,kernel_size=8,stride=4)\n",
    "        self.conv_layer2 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=4,stride=2)\n",
    "        self.conv_layer3 = nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,stride=1)\n",
    "        self.conv_layer4 = nn.Conv1d(in_channels=64,out_channels=512,kernel_size=7,stride=1)\n",
    "        self.lstm_layer = nn.LSTM(input_size=512,hidden_size=512,num_layers=1,batch_first=True)\n",
    "        self.adv = nn.Linear(in_features=512,out_features=self.out_size)\n",
    "        self.val = nn.Linear(in_features=512,out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,bsize,time_step,hidden_state,cell_state):\n",
    "        x = x.view(bsize*time_step,1,-1)\n",
    "        \n",
    "        conv_out = self.conv_layer1(x)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer2(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer3(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer4(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        # print(conv_out.shape) # [1,512,~]\n",
    "        \n",
    "        conv_out = conv_out.view(bsize, -1, 512)\n",
    "        \n",
    "        lstm_out = self.lstm_layer(conv_out,(hidden_state,cell_state))\n",
    "        out = lstm_out[0][:,time_step-1,:]\n",
    "        h_n = lstm_out[1][0]\n",
    "        c_n = lstm_out[1][1]\n",
    "        \n",
    "        adv_out = self.adv(out)\n",
    "        val_out = self.val(out)\n",
    "        \n",
    "        qout = val_out.expand(bsize,self.out_size) + (adv_out - adv_out.mean(dim=1).unsqueeze(dim=1).expand(bsize,self.out_size))\n",
    "        \n",
    "        return qout, (h_n,c_n)\n",
    "    \n",
    "    def init_hidden_states(self,bsize):\n",
    "        h = torch.zeros(1,bsize,512).float().to(device)\n",
    "        c = torch.zeros(1,bsize,512).float().to(device)\n",
    "        \n",
    "        return h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple network\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(6, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2:\n",
    "    def __init__(self):\n",
    "        self.agent = H2Y2_Agent()\n",
    "        self.env = H2Y2_env()\n",
    "        self.eps = 1.0 # 처음 epsilon\n",
    "        self.scores = []\n",
    "        self.end_times = []\n",
    "        \n",
    "    def dqn(self, n_episodes=10, max_t=10, eps_end=0.01, eps_decay=0.99):\n",
    "            \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            print(f\"{i_episode} episode\")\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            for t in range(1,max_t+1):\n",
    "                # print(f\"{t} time\")\n",
    "                action_index = self.agent.select_action(state, self.eps)\n",
    "                next_state, reward, done = self.env.step(state, action_index)\n",
    "                self.agent.step(state, action_index, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "            self.scores.append(score) # 하나의 episode에 대한 score(reward)를 저장\n",
    "            self.end_times.append(t) # 몇번의 step을 통해 done에 도달하는지 확인\n",
    "            self.eps = max(eps_end, eps_decay*self.eps) # decrease epsilon\n",
    "            # if np.mean(self.scores_window)>=200.0:\n",
    "            #     break\n",
    "            if i_episode % 5 == 0:\n",
    "                torch.save(self.agent.main_network.state_dict(),f'./model/{i_episode}_cpu_test.pt') \n",
    "        # model save\n",
    "        torch.save(self.agent.main_network.state_dict(),'./model/cpu_test.pt')\n",
    "        return np.mean(self.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:09<00:18,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:15<00:20,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:25<00:24,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:31<00:18,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:39<00:13,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:45<00:06,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.340935672514621"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2y2 = H2Y2()\n",
    "\n",
    "h2y2.dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.736842105263158,\n",
       " 9.210526315789476,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 7.140350877192983,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 7.257309941520469,\n",
       " 9.485380116959066]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2y2.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n",
      "0.9415204678362573\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "def main():\n",
    "    env = H2Y2_env()\n",
    "    agent = H2Y2_Agent()\n",
    "    state = env.reset()\n",
    "    print(len(state))\n",
    "    next_state, reward, done = env.step(state, 48)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    \n",
    "# 실험해보면 좋을 것\n",
    "# 1. 다음달에 새로운 데이터가 추가되었을 때도 잘 동작하는가 ( 데이터가 점진적으로 추가되었을 때 사용이 가능한가 )\n",
    "# 2. 그리드서치, 랜덤서치, 베이지안 서치랑 비교\n",
    "# 3. 데이터 도메인이 완전히 변경되었을 때도 lstm 레이어만 파인튜닝 튜닝하는 것으로 성능이 좋을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터를 불러와서 하이퍼파라미터 튜닝\n",
    "\n",
    "h2y2_test = H2Y2()\n",
    "weights = torch.load('model/CNN_LSTM_WEIGHTS_50epoch_10000max_t.pt', map_location='cpu')\n",
    "h2y2_test.agent.main_network.load_state_dict(weights)\n",
    "max_t = 1000\n",
    "scores = []\n",
    "\n",
    "for episode in range(1):\n",
    "    state = h2y2_test.env.reset()\n",
    "    for t in range(1,max_t+1):\n",
    "        action_index = h2y2_test.agent.select_action(state, 0.01)\n",
    "        next_state, reward, done = h2y2_test.env.step(state, action_index)\n",
    "        state = next_state\n",
    "        scores.append(reward)\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x140b76e73a0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('h2y2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53e991c9edeba9e61f5fc5dc21b635cf9d5ecc55b271a0f82c627d18cdfb2087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
