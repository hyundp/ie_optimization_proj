{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    \\n    더 나아가서,\\n    1. multi agent를 사용해야하나?\\n    \\n    state 설정\\n    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \\n    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\\n    따라서 state는 (729, 6)의 크기를 갖는다.\\n    \\n    deep q 이유\\n    만약 일반 q learning을 쓸 때, \\n    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\\n    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    정리\n",
    "    1. agent를 통해서 policy(랜덤값이 앱실론보다 작으면 랜덤 인덱스와 랜덤 action을, \n",
    "                            그렇지 않으면 내부 신경망(LSTM)에 state값을 넣어 \n",
    "                            가능한 모든 action에 대한 action_values(Q_values) 중 가장 큰 값에 해당하는 인덱스와 그 값을 반환한다.\n",
    "                            이때 사용되는 내부 신경망은 main_network이다.)\n",
    "        에 맞는 action_index와 action_value를 찾아낸다.\n",
    "    2. 찾아낸 action_index와 action_value를 env에 넣어서 RF 모델을 돌린 뒤 next_state(다음 action 집합)와 reward(정확도)를 찾아낸다.\n",
    "    3. memory(replay buffer)에 현재 state와 action_index, 그리고 위에서 도출된 next_state, reward를 넣는다.\n",
    "    4. replay buffer에서 원하는 batch size만큼 sample(state, action_index, reward, next_state)을 뽑아낸다.\n",
    "    5. sample의 state를 main_network에 넣어서 도출된 값들 중 action index에 맞는 값을 뽑고 이를 main Q 값으로 한다.\n",
    "    6. sample의 next_state를 target_network에 넣어서 값을 도출한 뒤, 그 중 가장 큰 값을 찾아낸다.\n",
    "    7. 위에서 찾아낸 값을 Q라고 한다면, reward + gamma(discount factor)*Q = target Q로 정한다.\n",
    "    8. mainQ와 targetQ를 loss function에 넣어서 내부 신경망 파라미터를 역전파로 업데이트 한다. 이때 옵티마이저도 사용된다.\n",
    "    9. 이 과정을 반복하여 main Q가 target Q에 가까워질 수 있게 한다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"    \n",
    "    더 나아가서,\n",
    "    1. multi agent를 사용해야하나?\n",
    "    \n",
    "    state 설정\n",
    "    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \n",
    "    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\n",
    "    따라서 state는 (729, 6)의 크기를 갖는다.\n",
    "    \n",
    "    deep q 이유\n",
    "    만약 일반 q learning을 쓸 때, \n",
    "    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\n",
    "    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "from itertools import product\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "class h2y2_RF_Model():\n",
    "    def __init__(self, cur_hyperparameter):\n",
    "        self.model = RandomForestClassifier(**cur_hyperparameter, random_state=42)\n",
    "        self.data = load_breast_cancer()\n",
    "        x_data = pd.DataFrame(self.data.data, columns=self.data.feature_names)\n",
    "        y_data = self.data.target\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.fit(self.train_x, self.train_y)\n",
    "        predict = self.model.predict(self.test_x)\n",
    "        return accuracy_score(self.test_y, predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_env:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initial\n",
    "        self.comb_config = [[-50,0,50], [-2,0,2], [-1,0,1], [-1,0,1], [-0.1,0,0.1], [-0.1,0,0.1]]\n",
    "        self.hyperparameter_list = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'min_impurity_decrease']\n",
    "        self.epsilon = 1e-3\n",
    "    \n",
    "    # 하이퍼파라미터의 범위를 제한해주는 함수 ver1\n",
    "    def check_bound(self, comb):        \n",
    "        comb_sum = comb[1] + comb[2]\n",
    "        if comb[0] == 'n_estimators':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'max_depth':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_samples_leaf':\n",
    "            if comb_sum > 0:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_samples_split':\n",
    "            if comb_sum > 1:\n",
    "                return int(comb_sum)\n",
    "        elif comb[0] == 'min_weight_fraction_leaf':\n",
    "            if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                return float(comb_sum)\n",
    "        elif comb[0] == 'min_impurity_decrease':\n",
    "            if comb_sum >= 0 and comb_sum <= 1:\n",
    "                return float(comb_sum)\n",
    "        return comb[1]\n",
    "    \n",
    "    # 하이퍼파라미터의 범위를 제한해주는 함수 ver2\n",
    "    def check_bound_ver2(self, comb):\n",
    "        sample_state = []\n",
    "        for i in range(len(comb[2])):\n",
    "            comb_sum = comb[1] + comb[2][i]\n",
    "            if comb[0] == 'n_estimators':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'max_depth':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_samples_leaf':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_samples_split':\n",
    "                if comb_sum > 1:\n",
    "                    sample_state.append(int(comb_sum))\n",
    "            elif comb[0] == 'min_weight_fraction_leaf':\n",
    "                if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                    sample_state.append(float(comb_sum))\n",
    "            elif comb[0] == 'min_impurity_decrease':\n",
    "                if comb_sum >= 0 and comb_sum <= 1:\n",
    "                    sample_state.append(float(comb_sum))\n",
    "            else:\n",
    "                sample_state.append(comb[1])\n",
    "        return sample_state\n",
    "    \n",
    "    # hyper-parameter vector를 받아서 다음으로 가능한 모든 조합을 반환해주는 함수\n",
    "    def make_state(self, cur_comb):\n",
    "        comb = list(product(*self.comb_config))\n",
    "        state = [tuple(map(self.check_bound, zip(self.hyperparameter_list, cur_comb, tuple_comb))) for tuple_comb in comb]\n",
    "        # print(state)\n",
    "        return state\n",
    "        \n",
    "    # q_value로 도출된 action_index가 어떤 observation을 가리키는지 확인한다.\n",
    "    def mapping_action(self, state, action_index):\n",
    "        all_state = list(product(*state))\n",
    "        # print(len(all_state))\n",
    "        return all_state[action_index]\n",
    "        \n",
    "    # hyper-parameter vector를 받아서 파라미터마다 가능한 값을 2차원으로 반환해주는 함수 ex)[[90, 100, 110], [2, 4, 6], [20, 25, 30], ...]\n",
    "    def make_state_ver2(self, cur_comb):\n",
    "        state = []\n",
    "        for name, cur, comb in zip(self.hyperparameter_list, cur_comb, self.comb_config):\n",
    "            state.append(self.check_bound_ver2([name,cur,comb]))\n",
    "        # print(state)\n",
    "        return state\n",
    "    \n",
    "    # env의 초기 state 설정, state는 hyper-parameter의 가능한 모든 조합으로 정의한다.\n",
    "    def reset(self):\n",
    "        init_hp = [random.randint(1, 100), random.randint(1, 100), random.randint(2, 100), random.randint(1, 100), random.random()/2, random.random()]\n",
    "        state = self.make_state(init_hp)\n",
    "        return state\n",
    "        \n",
    "    # action을 넣어서 next_state와 reward를 반환하는 함수\n",
    "    def step(self, state, action_index):\n",
    "        done = 0\n",
    "        cur_comb = state[action_index]\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state(cur_comb)\n",
    "        if reward == 1-self.epsilon:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "\n",
    "    # action을 넣어서 next_state와 reward를 반환하는 함수 ver2\n",
    "    def step_ver2(self, state, action_index):\n",
    "        done = 0\n",
    "        cur_comb = self.mapping_action(state, action_index) # state에 맞게 현재 조합 매핑\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state_ver2(cur_comb)\n",
    "        if reward == 1-self.epsilon:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.99 # discount factor\n",
    "        self.t_step = 0\n",
    "        self.learn_freq = 4\n",
    "        self.target_update_freq = 2000\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 6\n",
    "\n",
    "        self.main_network = Network(input_size=(729,6), out_size=729).float().to(device)\n",
    "        self.target_network = Network(input_size=(729,6), out_size=729).float().to(device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        \n",
    "        self.hidden_state, self.cell_state = self.main_network.init_hidden_states(bsize=1)\n",
    "        self.memory = ReplayBuffer(action_size = self.action_size, buffer_size = 10000, batch_size = self.batch_size, seed=42)\n",
    "        self.optimizer = optim.Adam(self.main_network.parameters(), lr = 0.01)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, eps=0.):\n",
    "        # \"내부의 신경망에 state를 넣어 모든 q_value를 뽑고, argmax로 선택\"\n",
    "        \n",
    "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0).to(device)\n",
    "        self.main_network.eval()\n",
    "        with torch.no_grad(): # 연산속도 증가\n",
    "            action_values, _ = self.main_network.forward(state, bsize=1, time_step=1, hidden_state=self.hidden_state, cell_state=self.cell_state)\n",
    "            # print(type(action_values)) # tuple\n",
    "        self.main_network.train()\n",
    "    \n",
    "        # q_value를 최대로 만드는 action의 인덱스를 선택한다.\n",
    "        if random.random() > eps:\n",
    "            max_index = torch.argmax(action_values)\n",
    "            return max_index\n",
    "        else:\n",
    "            random_index = random.choice(np.arange(self.action_size))\n",
    "            return random_index\n",
    "            \n",
    "    def step(self, state, action_index, reward, next_state, done):\n",
    "        # 메모리에 현재의 state, action_index, reward, next_state, done을 추가한다.\n",
    "        self.t_step += 1 \n",
    "        # print(self.t_step) # t_step은 episode가 바뀌어도 유지된다.\n",
    "        self.memory.add(state, action_index, reward, next_state, done)\n",
    "        \n",
    "        # target_update로 정해둔 step마다, target network의 파라미터를 업데이트 한다.\n",
    "        if (self.t_step % self.target_update_freq) == 0:\n",
    "            self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "            \n",
    "        # learn_freq로 정해둔 step마다, batch_size만큼의 샘플을 가지고 main_network를 학습시킨다.\n",
    "        if (self.t_step % self.learn_freq) == 0:\n",
    "            # batch_size만큼의 sample이 memory에 있으면 학습을 실행한다.\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "    \n",
    "    # main_network의 파라미터를 학습하는 함수\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, reward, next_states, dones = experiences\n",
    "        hidden_batch, cell_batch = self.main_network.init_hidden_states(bsize=self.batch_size)\n",
    "\n",
    "        # target_network를 통해 next_state에 대한 q_value 값을 도출하고, 그 중 max인 값을 선택한다.\n",
    "        Q_targets_next, _ = self.target_network.forward(next_states,bsize=self.batch_size, time_step=1, hidden_state=hidden_batch, cell_state=cell_batch)\n",
    "        Q_targets_next_max, __ = Q_targets_next.detach().max(dim=1)\n",
    "        Q_targets_next_max = Q_targets_next_max.view(-1,1)\n",
    "        # print(Q_targets_next_max.shape) # torch.Size([23328, 1]) -> torch.Size([32, 1])이 나와야함. -> 완료\n",
    "        \n",
    "        # q_value_target을 계산한다.\n",
    "        Q_targets = reward + (gamma * Q_targets_next_max)\n",
    "        # print(Q_targets.shape) # (32,1)\n",
    "\n",
    "        # main_network를 통해 현재 state와 action 대한 q_value를 도출한다.\n",
    "        Q_expected, _ = self.main_network.forward(states, bsize=self.batch_size, time_step=1, hidden_state=hidden_batch, cell_state=cell_batch)\n",
    "        # print(Q_expected.shape) # (32,729)\n",
    "        Q_expected_action = Q_expected.gather(dim=1, index = actions)\n",
    "        # print(Q_expected_action.shape) # (32,1)\n",
    "\n",
    "        loss = F.mse_loss(Q_expected_action, Q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.action_size = action_size #각각의 action의 차원\n",
    "        self.memory = deque(maxlen=buffer_size)  #버퍼의 최대 크기\n",
    "        self.batch_size = batch_size # 배치 사이즈\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed) # 랜덤 시드\n",
    "    \n",
    "    # states, actions, rewards, next_states, done을 replay memory에 저장하는 함수\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    # replay memory에서 이전 과정에서 저장된 states, actions, rewards, next_states, done을 랜덤 추출하는 함수\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([torch.as_tensor(e.state).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        # print(states.shape) # (32*729,6)\n",
    "        actions = torch.from_numpy(np.vstack([torch.as_tensor(e.action).cpu() for e in experiences if e is not None])).long().to(device)\n",
    "        # print(actions.shape) # (32,1)\n",
    "        rewards = torch.from_numpy(np.vstack([torch.as_tensor(e.reward).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        # print(rewards.shape) # (32,1)\n",
    "        next_states = torch.from_numpy(np.vstack([torch.as_tensor(e.next_state).cpu() for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([torch.as_tensor(e.done).cpu() for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        # print(dones.shape) # (32,1)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru도 비교해보면 좋다.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,out_size):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv1d(in_channels=1,out_channels=32,kernel_size=8,stride=4)\n",
    "        self.conv_layer2 = nn.Conv1d(in_channels=32,out_channels=64,kernel_size=4,stride=2)\n",
    "        self.conv_layer3 = nn.Conv1d(in_channels=64,out_channels=64,kernel_size=3,stride=1)\n",
    "        self.conv_layer4 = nn.Conv1d(in_channels=64,out_channels=512,kernel_size=7,stride=1)\n",
    "        self.lstm_layer = nn.LSTM(input_size=512,hidden_size=512,num_layers=1,batch_first=True)\n",
    "        self.adv = nn.Linear(in_features=512,out_features=self.out_size)\n",
    "        self.val = nn.Linear(in_features=512,out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,bsize,time_step,hidden_state,cell_state):\n",
    "        x = x.view(bsize*time_step,1,-1)\n",
    "        \n",
    "        conv_out = self.conv_layer1(x)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer2(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer3(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer4(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        # print(conv_out.shape) # [1,512,~]\n",
    "        \n",
    "        conv_out = conv_out.view(bsize, -1, 512)\n",
    "        \n",
    "        lstm_out = self.lstm_layer(conv_out,(hidden_state,cell_state))\n",
    "        out = lstm_out[0][:,time_step-1,:]\n",
    "        h_n = lstm_out[1][0]\n",
    "        c_n = lstm_out[1][1]\n",
    "        \n",
    "        adv_out = self.adv(out)\n",
    "        val_out = self.val(out)\n",
    "        \n",
    "        qout = val_out.expand(bsize,self.out_size) + (adv_out - adv_out.mean(dim=1).unsqueeze(dim=1).expand(bsize,self.out_size))\n",
    "        \n",
    "        return qout, (h_n,c_n)\n",
    "    \n",
    "    def init_hidden_states(self,bsize):\n",
    "        h = torch.zeros(1,bsize,512).float().to(device)\n",
    "        c = torch.zeros(1,bsize,512).float().to(device)\n",
    "        \n",
    "        return h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple network\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(6, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2:\n",
    "    def __init__(self):\n",
    "        self.agent = H2Y2_Agent()\n",
    "        self.env = H2Y2_env()\n",
    "        self.eps = 1.0 # 처음 epsilon\n",
    "        self.scores = []\n",
    "        self.end_times = []\n",
    "        \n",
    "    def dqn(self, n_episodes=10, max_t=10, eps_end=0.01, eps_decay=0.99):\n",
    "            \n",
    "        for i_episode in tqdm(range(1, n_episodes+1)):\n",
    "            print(f\"{i_episode} episode\")\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            for t in range(1,max_t+1):\n",
    "                # print(f\"{t} time\")\n",
    "                action_index = self.agent.select_action(state, self.eps)\n",
    "                next_state, reward, done = self.env.step(state, action_index)\n",
    "                self.agent.step(state, action_index, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break\n",
    "            self.scores.append(score) # 하나의 episode에 대한 score(reward)를 저장\n",
    "            self.end_times.append(t) # 몇번의 step을 통해 done에 도달하는지 확인\n",
    "            self.eps = max(eps_end, eps_decay*self.eps) # decrease epsilon\n",
    "            # if np.mean(self.scores_window)>=200.0:\n",
    "            #     break|\n",
    "            if i_episode % 5 == 0:\n",
    "                torch.save(self.agent.main_network.state_dict(),f'./model/{i_episode}_cpu_test.pt') \n",
    "        # model save\n",
    "        torch.save(self.agent.main_network.state_dict(),'./model/cpu_test.pt')\n",
    "        return np.mean(self.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:01<00:12,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:03<00:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:03<00:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:09<00:18,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:15<00:20,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:25<00:24,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:31<00:18,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:39<00:13,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:45<00:06,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 episode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.340935672514621"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "h2y2 = H2Y2()\n",
    "\n",
    "h2y2.dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.736842105263158,\n",
       " 9.210526315789476,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 7.140350877192983,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 6.3157894736842115,\n",
       " 7.257309941520469,\n",
       " 9.485380116959066]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2y2.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n",
      "0.9415204678362573\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "def main():\n",
    "    env = H2Y2_env()\n",
    "    agent = H2Y2_Agent()\n",
    "    state = env.reset()\n",
    "    print(len(state))\n",
    "    next_state, reward, done = env.step(state, 48)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    \n",
    "# 실험해보면 좋을 것\n",
    "# 1. 다음달에 새로운 데이터가 추가되었을 때도 잘 동작하는가 ( 데이터가 점진적으로 추가되었을 때 사용이 가능한가 )\n",
    "# 2. 그리드서치, 랜덤서치, 베이지안 서치랑 비교\n",
    "# 3. 데이터 도메인이 완전히 변경되었을 때도 lstm 레이어만 파인튜닝 튜닝하는 것으로 성능이 좋을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터를 불러와서 하이퍼파라미터 튜닝\n",
    "\n",
    "h2y2_test = H2Y2()\n",
    "weights = torch.load('model/CNN_LSTM_WEIGHTS_50epoch_10000max_t.pt', map_location='cpu')\n",
    "h2y2_test.agent.main_network.load_state_dict(weights)\n",
    "max_t = 500\n",
    "scores = []\n",
    "done_t = []\n",
    "\n",
    "for episode in range(1):\n",
    "    state = h2y2_test.env.reset()\n",
    "    for t in range(1,max_t+1):\n",
    "        action_index = h2y2_test.agent.select_action(state, 0.01)\n",
    "        next_state, reward, done = h2y2_test.env.step(state, action_index)\n",
    "        state = next_state\n",
    "        scores.append(reward)\n",
    "        done_t.append(done)\n",
    "        if done:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2809b887010>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('h2y2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53e991c9edeba9e61f5fc5dc21b635cf9d5ecc55b271a0f82c627d18cdfb2087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
