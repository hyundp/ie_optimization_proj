{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    할것\\n    1. state 설정 -> 현재 상태의 모든 가능한 하이퍼파라미터 조합 space\\n    2. state에서 action 도출 과정 표현\\n    3. replay memory 설정\\n    \\n    고민\\n    1. multi agent를 사용해야하나?\\n    \\n    state 설정\\n    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \\n    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\\n    따라서 state는 (729, 5)의 크기를 갖는다.\\n    \\n    deep q 이유\\n    만약 일반 q learning을 쓸 때, \\n    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\\n    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\\n    \\n    우리는 하나의 state당 729가지의 action이 생기고, 이러한 state\\n    각 하이퍼파라미터 당 10번의 tracking을 한다고 가정하면 ?? 얼만큼의 메모리가 소요된다고 할 수 있는가?\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    정리\n",
    "    1. agent를 통해서 policy(랜덤값이 앱실론보다 작으면 랜덤 인덱스와 랜덤 action을, \n",
    "                            그렇지 않으면 내부 신경망(LSTM)에 state값을 넣어 \n",
    "                            가능한 모든 action에 대한 action_values(Q_values) 중 가장 큰 값에 해당하는 인덱스와 그 값을 반환한다.\n",
    "                            이때 사용되는 내부 신경망은 main_network이다.)\n",
    "        에 맞는 action_index와 action_value를 찾아낸다.\n",
    "    2. 찾아낸 action_index와 action_value를 env에 넣어서 RF 모델을 돌린 뒤 next_state(다음 action 집합)와 reward(정확도)를 찾아낸다.\n",
    "    3. memory(replay buffer)에 현재 state와 action_index, 그리고 위에서 도출된 next_state, reward를 넣는다.\n",
    "    4. replay buffer에서 원하는 batch size만큼 sample(state, action_index, reward, next_state)을 뽑아낸다.\n",
    "    5. sample의 state를 main_network에 넣어서 도출된 값들 중 action index에 맞는 값을 뽑고 이를 main Q 값으로 한다.\n",
    "    6. sample의 next_state를 target_network에 넣어서 값을 도출한 뒤, 그 중 가장 큰 값을 찾아낸다.\n",
    "    7. 위에서 찾아낸 값을 Q라고 한다면, reward + gamma(discount factor)*Q = target Q로 정한다.\n",
    "    8. mainQ와 targetQ를 loss function에 넣어서 내부 신경망 파라미터를 역전파로 업데이트 한다. 이때 옵티마이저도 사용된다.\n",
    "    9. 이 과정을 반복하여 main Q가 target Q에 가까워질 수 있게 한다.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    할것\n",
    "    1. state 설정 -> 현재 상태의 모든 가능한 하이퍼파라미터 조합 space\n",
    "    2. state에서 action 도출 과정 표현\n",
    "    3. replay memory 설정\n",
    "    \n",
    "    고민\n",
    "    1. multi agent를 사용해야하나?\n",
    "    \n",
    "    state 설정\n",
    "    만약 넣은 state가 100,None,2,1,0,0(defalut) 라면, action value는 \n",
    "    3^6=729개의 조합에 대해 q-value를 뽑고 그 중 하나를 고르는 것.\n",
    "    따라서 state는 (729, 5)의 크기를 갖는다.\n",
    "    \n",
    "    deep q 이유\n",
    "    만약 일반 q learning을 쓸 때, \n",
    "    하나의 하이퍼파라미터 당 10개의 tracking을 한다고 가정하면, 10^6만큼의 q-table이 필요하게 된다.\n",
    "    따라서 시/공간 효율성을 위해 하이퍼파라미터 조합에 대한 q-value 값을 표현해주는 신경망을 학습시키는 것이 도움이 될 수 있다.\n",
    "    \n",
    "    우리는 하나의 state당 729가지의 action이 생기고, 이러한 state\n",
    "    각 하이퍼파라미터 당 10번의 tracking을 한다고 가정하면 ?? 얼만큼의 메모리가 소요된다고 할 수 있는가?\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import copy\n",
    "from itertools import product\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "class h2y2_RF_Model():\n",
    "    def __init__(self, cur_hyperparameter):\n",
    "        self.model = RandomForestClassifier(**cur_hyperparameter, random_state=42)\n",
    "        self.data = load_breast_cancer()\n",
    "        x_data = pd.DataFrame(self.data.data, columns=self.data.feature_names)\n",
    "        y_data = self.data.target\n",
    "        self.train_x, self.test_x, self.train_y, self.test_y = train_test_split(x_data, y_data, test_size=0.3, random_state=42)\n",
    "        \n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.fit(self.train_x, self.train_y)\n",
    "        predict = self.model.predict(self.test_x)\n",
    "        return accuracy_score(self.test_y, predict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_env:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # initial\n",
    "        self.comb_config = [[-50,0,50], [-2,0,2], [-1,0,1], [-1,0,1], [-0.1,0,0.1], [-0.1,0,0.1]]\n",
    "        self.hyperparameter_list = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf', 'min_weight_fraction_leaf', 'min_impurity_decrease', 'max_samples']\n",
    "    \n",
    "    \n",
    "    # 하이퍼파라미터의 범위를 제한해주는 함수\n",
    "    def check_bound(self, comb):        \n",
    "        comb_sum = comb[1] + comb[2]\n",
    "        if comb[0] == 'n_estimators':\n",
    "            if comb_sum > 0:\n",
    "                return comb_sum\n",
    "        elif comb[0] == 'max_depth':\n",
    "            if comb_sum > 0:\n",
    "                return comb_sum\n",
    "        elif comb[0] == 'min_samples_leaf':\n",
    "            if comb_sum > 0:\n",
    "                return comb_sum\n",
    "        elif comb[0] == 'min_samples_split':\n",
    "            if comb_sum > 1:\n",
    "                return comb_sum\n",
    "        elif comb[0] == 'min_weight_fraction_leaf':\n",
    "            if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                return comb_sum\n",
    "        elif comb[0] == 'min_impurity_decrease':\n",
    "            if comb_sum >= 0 and comb_sum <= 1:\n",
    "                return comb_sum\n",
    "        return comb[1]\n",
    "    \n",
    "    def check_bound_ver2(self, comb):\n",
    "        sample_state = []\n",
    "        for i in range(len(comb[2])):\n",
    "            comb_sum = comb[1] + comb[2][i]\n",
    "            if comb[0] == 'n_estimators':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(comb_sum)\n",
    "            elif comb[0] == 'max_depth':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(comb_sum)\n",
    "            elif comb[0] == 'min_samples_leaf':\n",
    "                if comb_sum > 0:\n",
    "                    sample_state.append(comb_sum)\n",
    "            elif comb[0] == 'min_samples_split':\n",
    "                if comb_sum > 1:\n",
    "                    sample_state.append(comb_sum)\n",
    "            elif comb[0] == 'min_weight_fraction_leaf':\n",
    "                if comb_sum >= 0 and comb_sum <= 0.5:\n",
    "                    sample_state.append(comb_sum)\n",
    "            elif comb[0] == 'min_impurity_decrease':\n",
    "                if comb_sum >= 0 and comb_sum <= 1:\n",
    "                    sample_state.append(comb_sum)\n",
    "            else:\n",
    "                sample_state.append(comb[1])\n",
    "        return sample_state\n",
    "    \n",
    "    # hyper-parameter vector를 받아서 다음으로 가능한 모든 조합을 반환해주는 함수\n",
    "    def make_state(self, cur_comb):\n",
    "        comb = list(product(*self.comb_config))\n",
    "        state = [tuple(map(self.check_bound, zip(self.hyperparameter_list, cur_comb, tuple_comb))) for tuple_comb in comb]\n",
    "        # print(state)\n",
    "        return state\n",
    "        \n",
    "    # q_value로 도출된 action_index가 어떤 observation을 가리키는지 확인한다.\n",
    "    def mapping_action(self, state, action_index):\n",
    "        all_state = list(product(*state))\n",
    "        print(len(all_state))\n",
    "        return all_state[action_index]\n",
    "        \n",
    "    # hyper-parameter vector를 받아서 파라미터마다 가능한 값을 2차원으로 반환해주는 함수 ex)[[90, 100, 110], [2, 4, 6], [20, 25, 30], ...]\n",
    "    def make_state_ver2(self, cur_comb):\n",
    "        state = []\n",
    "        for name, cur, comb in zip(self.hyperparameter_list, cur_comb, self.comb_config):\n",
    "            state.append(self.check_bound_ver2([name,cur,comb]))\n",
    "        # print(state)\n",
    "        return state\n",
    "    \n",
    "    # env의 초기 state 설정, state는 hyper-parameter의 가능한 모든 조합으로 정의한다.\n",
    "    def reset(self):\n",
    "        init_hp = [random.randint(1, 100), random.randint(1, 100), random.randint(2, 100), random.randint(1, 100), random.random()/2, random.random()]\n",
    "        state = self.make_state_ver2(init_hp)\n",
    "        return state\n",
    "        \n",
    "    # action을 넣어서 next_state와 reward를 반환하는 함수\n",
    "    def step(self, state, action_index):\n",
    "        done = 0\n",
    "        cur_comb = state[action_index]\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state(cur_comb)\n",
    "        if reward == 1:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def step_ver2(self, state, action_index):\n",
    "        # action을 넣어서 next_state와 reward를 반환하는 함수\n",
    "        done = 0\n",
    "        cur_comb = self.mapping_action(state, action_index) # state에 맞게 현재 조합 매핑\n",
    "        cur_hyperparameter = dict(zip(self.hyperparameter_list, cur_comb))\n",
    "        rf_model = h2y2_RF_Model(cur_hyperparameter)\n",
    "        reward = rf_model.evaluate()\n",
    "        next_state = self.make_state_ver2(cur_comb)\n",
    "        if reward == 1:\n",
    "            done = 1\n",
    "        return next_state, reward, done\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2_Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.99 # discount factor\n",
    "        self.t_step = 0\n",
    "        self.target_update = 4\n",
    "        self.batch_size = 32\n",
    "        self.action_size = 6\n",
    "        # self.main_network = Network(state_size,action_size).float().to(device)\n",
    "        # self.target_network = Network(state_size,action_size).float().to(device)\n",
    "        self.main_network = q_network\n",
    "        self.target_network = copy.deepcopy(self.main_network).eval()\n",
    "        self.memory = ReplayBuffer(action_size = self.action_size, buffer_size = 10000, batch_size = self.batch_size, seed=42)\n",
    "        self.optimizer = optim.Adam(self.main_network.parameters(), lr = 0.01)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, eps=0.):\n",
    "        # \"내부의 신경망에 state를 넣어 모든 q_value를 뽑고, argmax로 선택\"\n",
    "\n",
    "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0).to(device)\n",
    "        self.main_network.eval()\n",
    "        with torch.no_grad(): # 연산속도 증가\n",
    "            action_values = self.main_network(state)\n",
    "            # print(action_values)\n",
    "            # print(action_values.shape)\n",
    "        self.main_network.train()\n",
    "    \n",
    "        # q_value를 최대로 만드는 action의 인덱스를 선택한다.\n",
    "        if random.random() > eps:\n",
    "            max_index = torch.argmax(action_values.cpu().data.numpy())\n",
    "            return max_index\n",
    "        else:\n",
    "            random_index = random.choice(np.arange(self.action_size))\n",
    "            return random_index\n",
    "            \n",
    "    def step(self, state, action_index, reward, next_state, done):\n",
    "        # 메모리에 현재의 state, action_index, reward, next_state, done을 추가한다.\n",
    "        self.memory.add(state, action_index, reward, next_state, done)\n",
    "        \n",
    "        # target_update로 정해둔 step마다, batch_size만큼의 샘플을 가지고 main_network를 학습시킨다.\n",
    "        self.t_step = (self.t_step + 1) % self.target_update\n",
    "        if self.t_step == 0:\n",
    "            \n",
    "            # batch_size만큼의 sample이 memory에 있으면 학습을 실행한다.\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "    \n",
    "    # main_network의 파라미터를 학습하는 함수\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, reward, next_states, dones = experiences\n",
    "\n",
    "        # target_network를 통해 next_state에 대한 q_value 값을 도출하고, 그 중 max인 값을 선택한다.\n",
    "        Q_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # q_value_target을 계산한다.\n",
    "        Q_targets = reward + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        # main_network를 통해 현재 state와 action 대한 q_value를 도출한다.\n",
    "        Q_expected = self.main_network(states).gather(1, actions)\n",
    "\n",
    "\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "\n",
    "        self.action_size = action_size #각각의 action의 차원\n",
    "        self.memory = deque(maxlen=buffer_size)  #버퍼의 최대 크기\n",
    "        self.batch_size = batch_size # 배치 사이즈\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed) # 랜덤 시드\n",
    "    \n",
    "    # states, actions, rewards, next_states, done을 replay memory에 저장하는 함수\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    # replay memory에서 이전 과정에서 저장된 states, actions, rewards, next_states, done을 랜덤 추출하는 함수\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (243,5)의 input(state)이 들어와서 (243,1)의 output(q_values)을 도출하는 network\n",
    "# -> (5,1)을 넣어서 (243,1)을 도출하는 것은 성능이 어떤가?\n",
    "# q_value 쌍을 도출했을 때, 그것의 인덱스가 순서를 보장하는지 확인 필요 -> 의미는 내가 정하는 것이고 순서는 보장된다.\n",
    "# gru도 비교해보면 좋다.\n",
    "\n",
    "# 729 by 하는건 투머치한거같다\n",
    "# (3,1,6) \n",
    "\n",
    "# 신경망을 discrete하게 고정하는 것보다 real value로 확장시키는 것이 좋다.\n",
    "# lstm의 input은 (sample, time_step, feature) 로 이루어진다.\n",
    "# 이때 sample은 데이터가 들어옴에 따라 자동으로 정해진다. 따라서 time_step과 feature만 정해주면 된다.\n",
    "# time_step은 몇개의 데이터로 다음 데이터를 예측할지를 정한다. 우리는 하나의 파라미터당 3가지 경우의 수가 있기 때문에 3으로 한다.\n",
    "# feature은 총 파라미터 개수로 한다. 우리는 6개의 파라미터를 사용하므로 6으로 한다.\n",
    "# 따라서 lstm의 input_shape = (3,6) 이다.\n",
    "\n",
    "# 교수님 말씀은 [[90, 100, 110], [2, 4, 6], [20, 25, 30], ...] -> (6, 3)을 넣는걸 말씀하시는건가?\n",
    "# 내가 한 것은 이 들의 모든 조합을 만든 것이다.\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,out_size):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.conv_layer1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=8,stride=4)\n",
    "        self.conv_layer2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=4,stride=2)\n",
    "        self.conv_layer3 = nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,stride=1)\n",
    "        self.conv_layer4 = nn.Conv2d(in_channels=64,out_channels=512,kernel_size=7,stride=1)\n",
    "        self.lstm_layer = nn.LSTM(input_shape=512,hidden_size=512,num_layers=1,batch_first=True)\n",
    "        self.adv = nn.Linear(in_features=512,out_features=self.out_size)\n",
    "        self.val = nn.Linear(in_features=512,out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,bsize,time_step,hidden_state,cell_state):\n",
    "        x = x.view(bsize*time_step,1,self.input_size,self.input_size)\n",
    "        \n",
    "        conv_out = self.conv_layer1(x)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer2(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer3(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        conv_out = self.conv_layer4(conv_out)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        \n",
    "        conv_out = conv_out.view(bsize,time_step,512)\n",
    "        \n",
    "        lstm_out = self.lstm_layer(conv_out,(hidden_state,cell_state))\n",
    "        out = lstm_out[0][:,time_step-1,:]\n",
    "        h_n = lstm_out[1][0]\n",
    "        c_n = lstm_out[1][1]\n",
    "        \n",
    "        adv_out = self.adv(out)\n",
    "        val_out = self.val(out)\n",
    "        \n",
    "        qout = val_out.expand(bsize,self.out_size) + (adv_out - adv_out.mean(dim=1).unsqueeze(dim=1).expand(bsize,self.out_size))\n",
    "        \n",
    "        return qout, (h_n,c_n)\n",
    "    \n",
    "    def init_hidden_states(self,bsize):\n",
    "        h = torch.zeros(1,bsize,512).float().to(device)\n",
    "        c = torch.zeros(1,bsize,512).float().to(device)\n",
    "        \n",
    "        return h,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple network\n",
    "q_network = nn.Sequential(\n",
    "    nn.Linear(3, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2Y2:\n",
    "    def __init__(self):\n",
    "        self.agent = H2Y2_Agent()\n",
    "        self.env = H2Y2_env()\n",
    "        self.eps = 1.0 # 처음 epsilon\n",
    "        \n",
    "    def dqn(self, n_episodes=2000, max_t=1000, eps_end=0.01, eps_decay=0.995):\n",
    "            \n",
    "        for i_episode in range(1, n_episodes+1):\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            for t in range(max_t):\n",
    "                action_index = self.agent.select_action(state, self.eps)\n",
    "                next_state, reward, done = self.env.step_ver2(state, action_index)\n",
    "                self.agent.step(state, action_index, reward, next_state, done)\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    break \n",
    "            self.scores_window.append(score)       # save most recent score\n",
    "            self.scores.append(score)              # save most recent score\n",
    "            self.eps = max(eps_end, eps_decay*self.eps) # decrease epsilon\n",
    "            if np.mean(self.scores_window)>=200.0:\n",
    "                break\n",
    "        return np.mean(self.scores_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zkdlx\\miniconda3\\envs\\h2y2\\lib\\site-packages\\ipykernel_launcher.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23664\\3252599386.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mh2y2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23664\\617358750.py\u001b[0m in \u001b[0;36mdqn\u001b[1;34m(self, n_episodes, max_t, eps_end, eps_decay)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                 \u001b[0maction_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_ver2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23664\\2007333251.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# \"내부의 신경망에 state를 넣어 모든 q_value를 뽑고, argmax로 선택\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 연산속도 증가\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "h2y2 = H2Y2()\n",
    "\n",
    "\n",
    "h2y2.dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23112\\1739732496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 실험해보면 좋을 것\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23112\\1739732496.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH2Y2_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mH2Y2_Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23112\\882644641.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0minit_hp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_hp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23112\\882644641.py\u001b[0m in \u001b[0;36mmake_state\u001b[1;34m(self, cur_comb)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mcomb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomb_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_bound\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameter_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_comb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple_comb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtuple_comb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;31m# print(state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = H2Y2_env()\n",
    "    agent = H2Y2_Agent()\n",
    "    state = env.reset()\n",
    "    print(len(state))\n",
    "    next_state, reward, done = env.step(state, 48)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()\n",
    "    \n",
    "# 실험해보면 좋을 것\n",
    "# 1. 다음달에 새로운 데이터가 추가되었을 떄도 잘 동작하는가 ( 데이터가 점진적으로 추가되었을 때 사용이 가능한가 )\n",
    "# 2. 랜덤서치랑 베이지안 서치랑 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('h2y2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "53e991c9edeba9e61f5fc5dc21b635cf9d5ecc55b271a0f82c627d18cdfb2087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
